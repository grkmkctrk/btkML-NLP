# -*- coding: utf-8 -*-
"""NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bg4UXSUluHJ7HLuUaofqWVneyb4yKm6x
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

col_name = ['Review', 'Like']
yorumlar = pd.read_csv('/content/Restaurant_Reviews.csv', error_bad_lines=False)

import re
import nltk
from nltk.stem.porter import PorterStemmer
from nltk.corpus import stopwords

# ingilizce stopwords ler
dur = nltk.download('stopwords')

ps = PorterStemmer()

## PREPROCESSING
hepsi = []
for i in range(len(yorumlar)):
  # Noktalama isaretleri temizlenir (Punctiations are cleaned)
  yorum = re.sub('[^a-zA-Z]', ' ', yorumlar['Review'][i])
  # Butun harfker kucuk harf oldu (All letter became lowercase)
  yorum = yorum.lower()
  # Kelime kelime ayrildi (splited word by word)
  yorum = yorum.split()
  # Diyor ki eger kelime stopwords degilse o kelimenin kokunu bul
  # this line says if kelime is not a stopword then find the stem of the kelime
  # set icinde tekrar eden kelimleri bir kez barindiran bir kumedir | set is a set that include the repetitive words just one time 
  yorum = [ps.stem(kelime) for kelime in yorum if not kelime in set(stopwords.words('english'))]
  # join the yorum list but put a space between each 2 words
  yorum = ' '.join(yorum)
  hepsi.append(yorum)

## FEATURE EXTRACTION  
# BAG OF WORDS
from sklearn.feature_extraction.text import CountVectorizer
# max_feature : kac tane en cok kullanilan kelime alsin | how many most used words will get 
cv = CountVectorizer(max_features = 2000)
# sparse matrix 
X = cv.fit_transform(hepsi).toarray() # en cok kullanilan kelimelerin secildigi matris # y = f(x)
Y = yorumlar.iloc[:, 1].values # cikti kolonu

from sklearn.model_selection import train_test_split

# verinin yuzde 66 si antrenman icin kullanilsin kalan yuzde 33'u test edilsin diye ayrdik
# random_state rastsal ayirma icin kullaniliyor ayni degeri alan her kod ayni ayrimi yapar
x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)

# matrisleri SimpleImputer fonksiyonun istedigi hale getirdik
y_train = y_train.reshape(-1, 1)
y_test = y_test.reshape(-1, 1)

# nan verileri duzelttik
from sklearn.impute import SimpleImputer

imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')

y_train = imp.fit_transform(y_train)
y_test = imp.fit_transform(y_test)

#naive bayes ile tahmin ettik
from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB

gnb = GaussianNB()
gnb.fit(x_train, y_train)

mnb = MultinomialNB() # negatif data geldigi icin normalize edilmemis degerler ile kullandim
mnb.fit(x_train, y_train)

bnb = BernoulliNB()
bnb.fit(x_train, y_train)

y_predg = gnb.predict(x_test)
y_predm = mnb.predict(x_test)
y_predb = bnb.predict(x_test)

from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test, y_predm)
print (f'acc : {(cm[0, 0] + cm[1, 1])/sum(cm.reshape(-1))}')

